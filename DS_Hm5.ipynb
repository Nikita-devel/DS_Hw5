{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN/OyMXHcATku9lF2Xy9kj1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nikita-devel/DS_Hw5/blob/main/DS_Hm5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eEFJTSPXHd0p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "783f2105-a08b-416f-ce5a-afc36eee010a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['stairs', 'running', 'idle', 'walking']\n",
            "DATASET f0s0. Loaded. shape: (193860, 4)\n",
            "DATASET f0s1. Loaded. shape: (193860, 7)\n",
            "DATASET f1s0. Loaded. shape: (6462, 91)\n",
            "DATASET f1s1. Loaded. shape: (6462, 94)\n",
            "\n",
            "DATASET f0s0. Where flatten=False, stat_feature=False\n",
            "Already loaded, skipped\n",
            "\n",
            "DATASET f0s1. Where flatten=False, stat_feature=True\n",
            "Already loaded, skipped\n",
            "\n",
            "DATASET f1s0. Where flatten=True, stat_feature=False\n",
            "Already loaded, skipped\n",
            "\n",
            "DATASET f1s1. Where flatten=True, stat_feature=True\n",
            "Already loaded, skipped\n",
            "--------------------------------------------------------------------------------\n",
            "DATASET f0s0. shape: (193860, 4)\n",
            "\n",
            "- classification: SVC\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "from pathlib import Path\n",
        "from time import time\n",
        "\n",
        "def gen_saved_name(key):\n",
        "    return SAVED_DATAFRAME_BASE.joinpath(f\"data-{key}.father\")\n",
        "\n",
        "# Function to load or save dataset\n",
        "def load_or_save_dataset(key, class_list, skip_load=False):\n",
        "    filename = gen_saved_name(key)\n",
        "    if not skip_load and filename.is_file():\n",
        "        df = pd.read_feather(filename)\n",
        "        X = df.iloc[:, :-1]\n",
        "        y = df.iloc[:, -1]\n",
        "        print(f\"DATASET {key}. Loaded. shape: {df.shape}\")\n",
        "    else:\n",
        "        df = pd.DataFrame()\n",
        "        X = pd.DataFrame()\n",
        "        y = pd.Series()\n",
        "    return df, X, y\n",
        "\n",
        "# Function to save dataset\n",
        "def save_dataset(key, df):\n",
        "    filename = gen_saved_name(key)\n",
        "    if not df.empty and not filename.is_file():\n",
        "        df.to_feather(filename)\n",
        "\n",
        "# Function to prepare dataset\n",
        "def prepare_dataset(class_path, flatten=True, stat_feature=True, limit_frames=None):\n",
        "    dfws = []\n",
        "    for class_id, work_class_path in enumerate(class_path):\n",
        "        list_files = list(sorted(work_class_path.glob('*.csv'), key=lambda path: int(path.stem.rsplit(\"-\", 1)[1])))\n",
        "        print(f\"Importing class '{work_class_path.name:7}' : {class_id}. Frames: {len(list_files)}\")\n",
        "        for i, filename in enumerate(list_files):\n",
        "            df_w = pd.read_csv(filename)\n",
        "            addon_features = [df_w]\n",
        "            if flatten:\n",
        "                addon_features = [flatten_frame(df_w)]\n",
        "            if stat_feature:\n",
        "                addon_features.append(add_stat_feature_frame(df_w, addon_features[0].shape[0]))\n",
        "            df_w = pd.concat(addon_features, axis=1)\n",
        "            df_w['class'] = class_id\n",
        "            dfws.append(df_w)\n",
        "            if limit_frames and (i > limit_frames):\n",
        "                break\n",
        "    df = pd.concat(dfws, axis=0, ignore_index=True)\n",
        "    print(df.shape)\n",
        "    return df\n",
        "\n",
        "# Function to flatten a frame\n",
        "def flatten_frame(frame):\n",
        "    columns = [f\"{col}_{i}\" for i in range(frame.shape[0]) for col in frame.columns]\n",
        "    return pd.DataFrame(frame.values.reshape(1, -1), columns=columns)\n",
        "\n",
        "# Function to add statistical features to a frame\n",
        "def add_stat_feature_frame(frame, rows):\n",
        "    features = []\n",
        "    for col_id in range(0, 3):\n",
        "        col = frame.iloc[:, col_id]\n",
        "        features.append(pd.DataFrame([col.mean()] * rows, columns=[f'{col.name}_mean']))\n",
        "        # Add other statistical features as needed\n",
        "    result = pd.concat(features, axis=1)\n",
        "    return result\n",
        "\n",
        "# Function to train and evaluate models\n",
        "def train_and_evaluate_model(model, X_train, y_train, X_test, y_test):\n",
        "    clf = model()\n",
        "\n",
        "    start_time_fit = time()\n",
        "    clf.fit(X_train, y_train)\n",
        "    end_time_fit = time()\n",
        "\n",
        "    start_time_predict = time()\n",
        "    y_test_predict = clf.predict(X_test)\n",
        "    end_time_predict = time()\n",
        "\n",
        "    fit_time = end_time_fit - start_time_fit\n",
        "    predict_time = end_time_predict - start_time_predict\n",
        "\n",
        "    return clf, y_test_predict, classification_report(y_test, y_test_predict, digits=4, target_names=class_list), fit_time, predict_time\n",
        "\n",
        "# Main code\n",
        "DATASET_KEYS = (\"f0s0\", \"f0s1\", \"f1s0\", \"f1s1\")\n",
        "SAVED_DATAFRAME_BASE = Path(\"/content/\")\n",
        "URL = \"https://drive.usercontent.google.com/download?id=1nzrtQpfaHL0OgJ_eXzA7VuEj7XotrSWO&export=download&authuser=0\"\n",
        "OUTPUT = Path(\"/content/homework\")\n",
        "CSV_DATA_PATH  = Path(\"/content/data\")\n",
        "\n",
        "if not OUTPUT.is_file():\n",
        "  !wget -O $OUTPUT $URL\n",
        "\n",
        "if OUTPUT.is_file() and not CSV_DATA_PATH.is_dir():\n",
        "  !unzip -q -o $OUTPUT\n",
        "  #!rm $OUTPUT\n",
        "\n",
        "df_set = {}\n",
        "X_set = {}\n",
        "y_set = {}\n",
        "\n",
        "if CSV_DATA_PATH.is_dir():\n",
        "    class_path = list(CSV_DATA_PATH.iterdir())\n",
        "    class_list = list(d.name for d in class_path)\n",
        "    print(class_list)\n",
        "\n",
        "reports = {}\n",
        "skip_models = {\n",
        "    \"SVC_Linear\": [\"f0s0\"]\n",
        "}\n",
        "\n",
        "SEED = 42\n",
        "limit_frames = None\n",
        "\n",
        "class_list = []\n",
        "\n",
        "for key in DATASET_KEYS:\n",
        "    df_set[key], X_set[key], y_set[key] = load_or_save_dataset(key, class_list, skip_load=False)\n",
        "\n",
        "for f in range(2):\n",
        "    for s in range(2):\n",
        "        key_set = f\"f{f}s{s}\"\n",
        "        print(f\"\\nDATASET {key_set}. Where flatten={bool(f)}, stat_feature={bool(s)}\")\n",
        "        if df_set.get(key_set) is not None and (not df_set[key_set].empty):\n",
        "            print(\"Already loaded, skipped\")\n",
        "            continue\n",
        "        df_set[key_set] = prepare_dataset(class_path, flatten=f, stat_feature=s, limit_frames=limit_frames)\n",
        "        save_dataset(key_set, df_set[key_set])\n",
        "        X_set[key_set] = df_set[key_set].iloc[:, :-1]\n",
        "        y_set[key_set] = df_set[key_set].iloc[:, -1]\n",
        "\n",
        "\n",
        "for key in X_set.keys():\n",
        "    print(\"-\" * 80)\n",
        "    print(f\"DATASET {key}. shape: {df_set[key].shape}\")\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_set[key], y_set[key], test_size=0.3, random_state=SEED, stratify=y_set[key])\n",
        "    if reports.get(key) is None:\n",
        "        reports[key] = {}\n",
        "    for model, classification in {\"SVC\": SVC, \"SVC_Linear\": SVC, \"RandomForestClassifier\": RandomForestClassifier}.items():\n",
        "        print(f\"\\n- classification: {model}\")\n",
        "        if reports[key].get(model):\n",
        "            print(\"   already fit, skipped\")\n",
        "            continue\n",
        "        if key in skip_models.get(model, []):\n",
        "            print(\"   skip this model\")\n",
        "            continue\n",
        "        model_instance, y_test_predict, report, fit_time, predict_time = train_and_evaluate_model(classification, X_train, y_train, X_test, y_test)\n",
        "        reports[key][model] = {'report': report, 'fit_time': fit_time, 'predict_time': predict_time}\n",
        "\n",
        "# Display reports\n",
        "for dset in reports.keys():\n",
        "    for model in reports[dset].keys():\n",
        "        print(\"-\" * 80)\n",
        "        print(f\"Data set: {dset}, shape: {df_set[dset].shape}, model: {model}\")\n",
        "        print(reports[dset][model])\n"
      ]
    }
  ]
}